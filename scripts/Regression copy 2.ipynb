{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib_resources\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from matplotlib.pylab import (\n",
    "    figure,\n",
    "    grid,\n",
    "    legend,\n",
    "    loglog,\n",
    "    semilogx,\n",
    "    show,\n",
    "    subplot,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    ")\n",
    "from scipy.io import loadmat\n",
    "from sklearn import model_selection\n",
    "\n",
    "from dtuimldmtools import rlr_validate\n",
    "\n",
    "\n",
    "filename = importlib_resources.files(\"dtuimldmtools\").joinpath(\"data/body.mat\")\n",
    "\n",
    "\n",
    "mat_data = loadmat(filename)\n",
    "X = mat_data[\"X\"]\n",
    "y = mat_data[\"y\"].squeeze()\n",
    "attributeNames = [name[0] for name in mat_data[\"attributeNames\"][0]]\n",
    "N, M = X.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_load import getTargets\n",
    "from data_standardization import X as X_num, missing_values as missing_values_num\n",
    "from data_encoding import X as X_cat, missing_values as missing_values_cat\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import model_selection\n",
    "from dtuimldmtools import rlr_validate\n",
    "from data_load import categorical_features, numerical_features\n",
    "from matplotlib.pylab import (\n",
    "    figure,\n",
    "    grid,\n",
    "    legend,\n",
    "    loglog,\n",
    "    semilogx,\n",
    "    show,\n",
    "    subplot,\n",
    "    title,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    ")\n",
    "\n",
    "\n",
    "missing_values = list(set(missing_values_num).union(set(missing_values_cat)))\n",
    "\n",
    "X = pd.concat([X_num, X_cat], axis=1)\n",
    "y = getTargets()\n",
    "\n",
    "X = X.drop(missing_values)\n",
    "y = y.drop(missing_values)\n",
    "\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "# Standardize X to make sure X_cat is also standardized, and to make sure that the mean and std of each column is 0 and 1 even without the rows with missing values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Change X and y into a arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y).squeeze()\n",
    "\n",
    "N, M = X.shape\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "[[ 0.31928084  8.1240384  -0.1754116  ... -0.43514263 -0.25197632\n",
      "  -0.24061325]\n",
      " [ 0.72056014  8.1240384  -0.1754116  ... -0.43514263 -0.25197632\n",
      "  -0.24061325]\n",
      " [ 0.72056014  8.1240384  -0.1754116  ... -0.43514263 -0.25197632\n",
      "  -0.24061325]\n",
      " ...\n",
      " [ 1.247495   -0.12309149 -0.1754116  ... -0.43514263 -0.25197632\n",
      "   4.15604707]\n",
      " [ 1.3369681  -0.12309149 -0.1754116  ... -0.43514263 -0.25197632\n",
      "   4.15604707]\n",
      " [ 1.35068945 -0.12309149 -0.1754116  ... -0.43514263 -0.25197632\n",
      "   4.15604707]]\n",
      "y\n",
      "[ 3  3  1  2  2  2  1  1  1  2  0  0  0  1  0  0  0  2  1  0  1  1  1  1\n",
      "  1  1  1 -1  3  2  2  1  1  1  0  0  0  0  0  0  0  1  0  2  0  0  0  1\n",
      "  1  1  1  1  3  3  3  3  1  0  1  0  0  0  0  0 -1 -1  0 -1 -1  3  0  1\n",
      "  1  2  2  2  1  3  3  3  3  3  1  1  1 -1  1  1  1  1  1  1  1  1  1  2\n",
      "  0  0  0  0  0  3  3  1  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1\n",
      " -1  3  3  3  3  3  0  2  3  2  3  2  3  2  2  2  2  0  0  0  0  0  0  0\n",
      "  0  0  1  1  1  0  0  0  0  0  0  0  0  0  0  1  1  1  1  2  2  2  2  2\n",
      "  2 -1 -1 -1 -1 -1  3  3 -1 -1  2  2  2  2  2  2  2  3  3  0  0  0 -2 -1\n",
      " -2 -1 -2 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Explore what is X and y\n",
    "print(\"X\")\n",
    "print(X)\n",
    "print(\"y\")\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (25,) into shape (24,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 43\u001b[0m\n\u001b[1;32m     32\u001b[0m (\n\u001b[1;32m     33\u001b[0m     opt_val_err,\n\u001b[1;32m     34\u001b[0m     opt_lambda,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     test_err_vs_lambda,\n\u001b[1;32m     38\u001b[0m ) \u001b[38;5;241m=\u001b[39m rlr_validate(X_train, y_train, lambdas, internal_cross_validation)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Standardize outer fold based on training set, and save the mean and standard\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# deviations since they're part of the model (they would be needed for\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# making new predictions) - for brevity we won't always store these in the scripts\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m mu[k, :] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(X_train[:, \u001b[38;5;241m1\u001b[39m:], \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     44\u001b[0m sigma[k, :] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(X_train[:, \u001b[38;5;241m1\u001b[39m:], \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     46\u001b[0m X_train[:, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m=\u001b[39m (X_train[:, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m-\u001b[39m mu[k, :]) \u001b[38;5;241m/\u001b[39m sigma[k, :]\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (25,) into shape (24,)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add offset attribute\n",
    "X = np.concatenate((np.ones((X.shape[0], 1)), X), 1)\n",
    "attributeNames = [u\"Offset\"] + numerical_features + categorical_features\n",
    "M = M + 1\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = 5\n",
    "CV = model_selection.KFold(K, shuffle=True)\n",
    "# CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Values of lambda\n",
    "lambdas = np.power(10.0, range(-5, 9))\n",
    "\n",
    "# Initialize variables\n",
    "# T = len(lambdas)\n",
    "Error_train = np.empty((K, 1))\n",
    "Error_test = np.empty((K, 1))\n",
    "Error_train_rlr = np.empty((K, 1))\n",
    "Error_test_rlr = np.empty((K, 1))\n",
    "Error_train_nofeatures = np.empty((K, 1))\n",
    "Error_test_nofeatures = np.empty((K, 1))\n",
    "w_rlr = np.empty((M, K))\n",
    "mu = np.empty((K, M - 1))\n",
    "sigma = np.empty((K, M - 1))\n",
    "w_noreg = np.empty((M, K))\n",
    "\n",
    "k = 0\n",
    "for train_index, test_index in CV.split(X, y):\n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    internal_cross_validation = 10\n",
    "\n",
    "    (\n",
    "        opt_val_err,\n",
    "        opt_lambda,\n",
    "        mean_w_vs_lambda,\n",
    "        train_err_vs_lambda,\n",
    "        test_err_vs_lambda,\n",
    "    ) = rlr_validate(X_train, y_train, lambdas, internal_cross_validation)\n",
    "\n",
    "    # Standardize outer fold based on training set, and save the mean and standard\n",
    "    # deviations since they're part of the model (they would be needed for\n",
    "    # making new predictions) - for brevity we won't always store these in the scripts\n",
    "    mu[k, :] = np.mean(X_train[:, 1:], 0)\n",
    "    sigma[k, :] = np.std(X_train[:, 1:], 0)\n",
    "\n",
    "    X_train[:, 1:] = (X_train[:, 1:] - mu[k, :]) / sigma[k, :]\n",
    "    X_test[:, 1:] = (X_test[:, 1:] - mu[k, :]) / sigma[k, :]\n",
    "\n",
    "    Xty = X_train.T @ y_train\n",
    "    XtX = X_train.T @ X_train\n",
    "\n",
    "    # Compute mean squared error without using the input data at all\n",
    "    Error_train_nofeatures[k] = (\n",
    "        np.square(y_train - y_train.mean()).sum(axis=0) / y_train.shape[0]\n",
    "    )\n",
    "    Error_test_nofeatures[k] = (\n",
    "        np.square(y_test - y_test.mean()).sum(axis=0) / y_test.shape[0]\n",
    "    )\n",
    "\n",
    "    # Estimate weights for the optimal value of lambda, on entire training set\n",
    "    lambdaI = opt_lambda * np.eye(M)\n",
    "    lambdaI[0, 0] = 0  # Do no regularize the bias term\n",
    "    w_rlr[:, k] = np.linalg.solve(XtX + lambdaI, Xty).squeeze()\n",
    "    # Compute mean squared error with regularization with optimal lambda\n",
    "    Error_train_rlr[k] = (\n",
    "        np.square(y_train - X_train @ w_rlr[:, k]).sum(axis=0) / y_train.shape[0]\n",
    "    )\n",
    "    Error_test_rlr[k] = (\n",
    "        np.square(y_test - X_test @ w_rlr[:, k]).sum(axis=0) / y_test.shape[0]\n",
    "    )\n",
    "\n",
    "    # Estimate weights for unregularized linear regression, on entire training set\n",
    "    w_noreg[:, k] = np.linalg.solve(XtX, Xty).squeeze()\n",
    "    # Compute mean squared error without regularization\n",
    "    Error_train[k] = (\n",
    "        np.square(y_train - X_train @ w_noreg[:, k]).sum(axis=0) / y_train.shape[0]\n",
    "    )\n",
    "    Error_test[k] = (\n",
    "        np.square(y_test - X_test @ w_noreg[:, k]).sum(axis=0) / y_test.shape[0]\n",
    "    )\n",
    "    # OR ALTERNATIVELY: you can use sklearn.linear_model module for linear regression:\n",
    "    # m = lm.LinearRegression().fit(X_train, y_train)\n",
    "    # Error_train[k] = np.square(y_train-m.predict(X_train)).sum()/y_train.shape[0]\n",
    "    # Error_test[k] = np.square(y_test-m.predict(X_test)).sum()/y_test.shape[0]\n",
    "\n",
    "    # Display the results for the last cross-validation fold\n",
    "    if k == K - 1:\n",
    "        figure(k, figsize=(12, 8))\n",
    "        subplot(1, 2, 1)\n",
    "        semilogx(lambdas, mean_w_vs_lambda.T[:, 1:], \".-\")  # Don't plot the bias term\n",
    "        xlabel(\"Regularization factor\")\n",
    "        ylabel(\"Mean Coefficient Values\")\n",
    "        grid()\n",
    "        # You can choose to display the legend, but it's omitted for a cleaner\n",
    "        # plot, since there are many attributes\n",
    "        # legend(attributeNames[1:], loc='best')\n",
    "\n",
    "        subplot(1, 2, 2)\n",
    "        title(\"Optimal lambda: 1e{0}\".format(np.log10(opt_lambda)))\n",
    "        loglog(\n",
    "            lambdas, train_err_vs_lambda.T, \"b.-\", lambdas, test_err_vs_lambda.T, \"r.-\"\n",
    "        )\n",
    "        xlabel(\"Regularization factor\")\n",
    "        ylabel(\"Squared error (crossvalidation)\")\n",
    "        legend([\"Train error\", \"Validation error\"])\n",
    "        grid()\n",
    "\n",
    "    # To inspect the used indices, use these print statements\n",
    "    # print('Cross validation fold {0}/{1}:'.format(k+1,K))\n",
    "    # print('Train indices: {0}'.format(train_index))\n",
    "    # print('Test indices: {0}\\n'.format(test_index))\n",
    "\n",
    "    k += 1\n",
    "\n",
    "show()\n",
    "# Display results\n",
    "print(\"Linear regression without feature selection:\")\n",
    "print(\"- Training error: {0}\".format(Error_train.mean()))\n",
    "print(\"- Test error:     {0}\".format(Error_test.mean()))\n",
    "print(\n",
    "    \"- R^2 train:     {0}\".format(\n",
    "        (Error_train_nofeatures.sum() - Error_train.sum())\n",
    "        / Error_train_nofeatures.sum()\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"- R^2 test:     {0}\\n\".format(\n",
    "        (Error_test_nofeatures.sum() - Error_test.sum()) / Error_test_nofeatures.sum()\n",
    "    )\n",
    ")\n",
    "print(\"Regularized linear regression:\")\n",
    "print(\"- Training error: {0}\".format(Error_train_rlr.mean()))\n",
    "print(\"- Test error:     {0}\".format(Error_test_rlr.mean()))\n",
    "print(\n",
    "    \"- R^2 train:     {0}\".format(\n",
    "        (Error_train_nofeatures.sum() - Error_train_rlr.sum())\n",
    "        / Error_train_nofeatures.sum()\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"- R^2 test:     {0}\\n\".format(\n",
    "        (Error_test_nofeatures.sum() - Error_test_rlr.sum())\n",
    "        / Error_test_nofeatures.sum()\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Weights in last fold:\")\n",
    "for m in range(M):\n",
    "    print(\"{:>15} {:>15}\".format(attributeNames[m], np.round(w_rlr[m, -1], 2)))\n",
    "\n",
    "print(\"Ran Exercise 8.1.1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
